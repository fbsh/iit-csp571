{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236b9e33-2782-4e7d-9e21-274d79038e9c",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fea2a9-4e2f-4d3b-a766-b2e17f3ed280",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e36a1-ff0e-4753-9211-254da91bc41d",
   "metadata": {},
   "source": [
    "When performing best subset selection, the model with $k$ predictors is the model with the smallest $R S S$ among all the $C_{p}^{k}$ models with $k$ predictors. When performing forward stepwise selection, the model with $k$ predictors is the model with the smallest $R S S$ among the $p-k$ models which augment the predictors in $\\mathcal{M}_{k-1}$ with one additional predictor. When performing backward stepwise selection, the model with $k$ predictors is the model with the smallest RSS among the $k$ models which contains all but one of the predictors in $\\mathcal{M}_{k+1} .$ So, the model with $k$ predictors which has the smallest training $R S S$ is the one obtained from best subset selection as it is the one selected among all $k$ predictors models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f8212-2710-4d7d-bee8-7d2f9ddf2c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab912a-3403-4624-abf4-5ebd963fac80",
   "metadata": {},
   "source": [
    "Best subset selection may have the smallest test RSS because it takes into account more models than the other methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eeb1c-dbbc-492d-9c34-aa68f121f3a4",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862a681-392a-4a8c-9730-d8798066769d",
   "metadata": {},
   "source": [
    "### i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbe43f-12b5-455e-b8a3-81fef803d788",
   "metadata": {},
   "source": [
    "True. The model with $(k+1)$ predictors is obtained by augmenting the predictors in the model with $k$ predictors with one additional predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74e240-f286-4293-abf9-f27fcde7e039",
   "metadata": {},
   "source": [
    "### ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a15a9-b589-4ddd-9a2b-9525be7be02f",
   "metadata": {},
   "source": [
    "True. The model with $k$ predictors is obtained by removing one predictor from the model with $(k+1)$ predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd98e3-a9d3-4cd4-b3bf-663ecfb9bc1f",
   "metadata": {},
   "source": [
    "### iii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15436c7-882c-4a61-a337-afc7e8e5151e",
   "metadata": {},
   "source": [
    "False. There is no direct link between the models obtained from forward and backward selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b5671-9084-4475-9dcd-32795aa8a6d2",
   "metadata": {},
   "source": [
    "### iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438602e6-cfce-4f66-b4fd-de73b9a27e6a",
   "metadata": {},
   "source": [
    "False. There is no direct link between the models obtained from forward and backward selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca6b68-9108-4a92-83bd-c5af3226d5c4",
   "metadata": {},
   "source": [
    "### v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607018a-1050-48aa-a1d6-37cd568ae585",
   "metadata": {},
   "source": [
    "False. The model with $(k+1)$ predictors is obtained by selecting among all possible models with $(k+1)$ predictors, and so does not necessarily contain all the predictors selected for the $k$-variable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221943a0-6023-4619-8e54-66bf341534ee",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32fbd8-fb3d-423e-b799-a7aaf569acc8",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e3743-f021-47c7-ab0a-5cbaf4e3ca27",
   "metadata": {},
   "source": [
    "The lasso is less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d490-a391-4fc5-8041-62bb9089c6ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114123b6-77ae-42d8-862e-80e029813608",
   "metadata": {},
   "source": [
    "Same as lasso, ridge regression is less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce0d00-910a-48ee-aa46-dd31c4fa6284",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67546e9-4375-4871-bace-2b5b8e782268",
   "metadata": {},
   "source": [
    "Non-linear methods are more flexible and will give improved prediction accuracy when their increase in variance are less than their decrease in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d869f-52f8-41df-8573-afbd6c82735d",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ad1ee-7dab-49fc-bb26-3e163f4febed",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd09269-659f-4e06-bca3-73a765557d37",
   "metadata": {},
   "source": [
    "Steadily decrease. As we increase $s$ from 0, we are restricting the $\\beta_{j}$ coefficients less and less (the coefficients will increase to their least squares estimates), and so the model is becoming more and more flexible which provokes a steady decrease in the training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbdf77-ec45-4d97-bd58-95570ed8aff5",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49befcd9-b11b-4525-a4a2-5acc04a07530",
   "metadata": {},
   "source": [
    "Decrease initially, and then eventually start increasing in a $U$ shape. As we increase $s$ from 0 , we are restricting the $\\beta_{j}$ coefficients less and less (the in the test RSS before increasing again after that in a typical U shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59f3a3-0a6b-4d03-98b1-2a3ec4038b07",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f889b-f855-4d46-9005-0c7b891773e3",
   "metadata": {},
   "source": [
    "Steadily increase. As we increase $s$ from 0 , we are restricting the $\\beta_{j}$ coefficients less and less (the coefficients will increase to their least squares estimates), and so the model is becoming more and more flexible which provokes a steady increase in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf219b-113a-4309-8ed6-2ab60b9f6adb",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87a7bf-1465-4c12-a95b-510c9b01a0c9",
   "metadata": {},
   "source": [
    "Steadily decrease. As we increase $s$ from 0 , we are restricting the $\\beta_{j}$ coefficients less and less (the coefficients will increase to their least squares estimates), and so the model is becoming more and more flexible which provokes a steady decrease in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978e782-7d05-4b86-8f66-36dceec0943f",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81e5c8-65ae-4589-8835-ed07b6e60bb6",
   "metadata": {},
   "source": [
    "Remain constant. By definition, the irreducible error is independant of the model, and consequently independant of the value of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aee695-a8e5-4f0d-b2d1-046be0ee6a27",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de519379-cf8a-44f2-92c4-ba1d7784a83b",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a30aa8-784e-4e2b-ae09-93dfe9d5283c",
   "metadata": {},
   "source": [
    "Steadily increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac765b-06b6-47df-9337-9742d40c2e12",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b89ada-0c39-4d47-a126-59a1ec9f8c5e",
   "metadata": {},
   "source": [
    "Decrease initially, and then eventually start increasing in a U shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de8833-321f-4ea2-83c2-c2ca81283bd8",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd66d-6d6f-41a0-b3f5-37c4cac7aae1",
   "metadata": {},
   "source": [
    "Steadily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dadef5-c27f-4ee1-a55d-174939cd786a",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdda9b-5303-4746-91ce-6c2f7a6000c1",
   "metadata": {},
   "source": [
    "Steadily increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1d88a-62e1-47b9-86d1-cb1d31168769",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d51236-c1e1-40de-a623-e0b3dc0668e1",
   "metadata": {},
   "source": [
    "Remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345613e-6425-465e-9cda-e5b0f71fb670",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56322d7-3d14-4df1-8904-940accf081be",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da5067-f27a-4689-baa3-c68492c3ef00",
   "metadata": {},
   "source": [
    "According to this setting $\\left(x_{11}=x_{12}=x_{1}\\right.$ and $\\left.x_{21}=x_{22}=x_{2}\\right)$, the ridge regression problem seeks to minimize\n",
    "$$\n",
    "\\left(y_{1}-\\hat{\\beta}_{1} x_{1}-\\hat{\\beta}_{2} x_{1}\\right)^{2}+\\left(y_{2}-\\hat{\\beta}_{1} x_{2}-\\hat{\\beta}_{2} x_{2}\\right)^{2}+\\lambda\\left(\\hat{\\beta}_{1}^{2}+\\hat{\\beta}_{2}^{2}\\right) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49151b9-d732-4e8f-b233-c2a2af667f0c",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ad8d1-c903-4ea2-bc64-c2b692248a72",
   "metadata": {},
   "source": [
    "By taking the derivatives of the above expression with respect to $\\hat{\\beta}_{1}$ and $\\hat{\\beta}_{2}$ and setting them equal to 0 , we obtain respectively\n",
    "$$\n",
    "\\hat{\\beta}_{1}\\left(x_{1}^{2}+x_{2}^{2}+\\lambda\\right)+\\hat{\\beta}_{2}\\left(x_{1}^{2}+x_{2}^{2}\\right)=y_{1} x_{1}+y_{2} x_{2}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\hat{\\beta}_{1}\\left(x_{1}^{2}+x_{2}^{2}\\right)+\\hat{\\beta}_{2}\\left(x_{1}^{2}+x_{2}^{2}+\\lambda\\right)=y_{1} x_{1}+y_{2} x_{2} .\n",
    "$$\n",
    "By substracting the two expressions above we get $\\hat{\\beta}_{1}=\\hat{\\beta}_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac380a-3575-4d16-a731-c71bd354b47a",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302ee95-0f63-4912-aa4c-cd77f7305026",
   "metadata": {},
   "source": [
    "According to this setting $\\left(x_{11}=x_{12}=x_{1}\\right.$ and $\\left.x_{21}=x_{22}=x_{2}\\right)$, the lasso optimization problem seeks to minimize\n",
    "$$\n",
    "\\left(y_{1}-\\hat{\\beta}_{1} x_{1}-\\hat{\\beta}_{2} x_{1}\\right)^{2}+\\left(y_{2}-\\hat{\\beta}_{1} x_{2}-\\hat{\\beta}_{2} x_{2}\\right)^{2}+\\lambda\\left(\\left|\\hat{\\beta}_{1}\\right|+\\left|\\hat{\\beta}_{2}\\right|\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030715a9-ca5d-4b86-8f12-7e8014366a92",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c56627-754d-40fa-b5e6-48340ecd388a",
   "metadata": {},
   "source": [
    "The lasso optimization problem\n",
    "$$\n",
    "\\left(y_{1}-\\hat{\\beta}_{1} x_{1}-\\hat{\\beta}_{2} x_{1}\\right)^{2}+\\left(y_{2}-\\hat{\\beta}_{1} x_{2}-\\hat{\\beta}_{2} x_{2}\\right)^{2} \\text { subject to }\\left|\\hat{\\beta}_{1}\\right|+\\left|\\hat{\\beta}_{2}\\right| \\leq s .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cdde5-c667-453a-8883-16efc1e86bd3",
   "metadata": {},
   "source": [
    "Using the setting of this problem $\\left(x_{11}=x_{12}=x_{1}, x_{21}=x_{22}=x_{2}, x_{1}+x_{2}=0\\right.$ and $\\left.y_{1}+y_{2}=0\\right)$, we have to minimize the expression\n",
    "$$\n",
    "2\\left[y_{1}-\\left(\\hat{\\beta}_{1}+\\hat{\\beta}_{2}\\right) x_{1}\\right]^{2} \\geq 0 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c2b7e-8052-4c10-b9bb-9ca452bcbf9a",
   "metadata": {},
   "source": [
    "The solution for this optimization problem will be : $\\hat{\\beta}_{1}+\\hat{\\beta}_{2}=y_{1} / x_{1}$. Geometrically, this is a line parallel to the edge of the diamond of the constraints. Now, solutions to the lasso optimization problem are contours of the function $\\left[y_{1}-\\left(\\hat{\\beta}_{1}+\\hat{\\beta}_{2}\\right) x_{1}\\right]^{2}$ that intersects the diamond of the constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b87f33-d6f6-43eb-8722-a08fcdcfb63b",
   "metadata": {},
   "source": [
    "So, the entire edge $\\hat{\\beta}_{1}+\\hat{\\beta}_{2}=s$ (as is the edge $\\hat{\\beta}_{1}+\\hat{\\beta}_{2}=-s$ ) is a potential solution to the lasso optimization problem. Thus, the lasso optimization problem has a whole set of solutions instead of a unique one:\n",
    "$$\n",
    "\\left\\{\\left(\\hat{\\beta}_{1}, \\hat{\\beta}_{2}\\right): \\hat{\\beta}_{1}+\\hat{\\beta}_{2}=s \\text { with } \\hat{\\beta}_{1}, \\hat{\\beta}_{2} \\geq 0 \\text { and } \\hat{\\beta}_{1}+\\hat{\\beta}_{2}=-s \\text { with } \\hat{\\beta}_{1}, \\hat{\\beta}_{2} \\leq 0\\right\\} .\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
